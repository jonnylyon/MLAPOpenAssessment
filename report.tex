\documentclass[a4paper,11pt]{article}
\usepackage[cm]{fullpage}
\usepackage{bchart}
\usepackage{pgfplots}
\usepackage{listings}

\title{MLAP Open Assessment A}
\author{Y6186228}
\date{14th May 2014}

\pgfplotsset{compat=1.10}

\newcommand*{\foa}[1]{f(\theta) = \theta_0 + \theta_1 {#1}}
\newcommand*{\fa}[1]{$\foa{#1}$}

\newcommand*{\fob}[2]{f(\theta) = \theta_0 + \theta_1 {#1} + \theta_2 {#2}}
\newcommand*{\fb}[2]{$\fob{#1}{#2}$}

\newcommand*{\foc}[3]{f(\theta) = \theta_0 + \theta_1 {#1} + \theta_2 {#2} + \theta_3 {#3}}
\newcommand*{\fc}[3]{$\foc{#1}{#2}{#3}$}

\newcommand*{\fod}[4]{f(\theta) = \theta_0 + \theta_1 {#1} + \theta_2 {#2} + \theta_3 {#3} + \theta_4 {#4}}
\newcommand*{\fd}[4]{$\fod{#1}{#2}{#3}{#4}$}

\newcommand*{\foe}[5]{f(\theta) = \theta_0 + \theta_1 {#1} + \theta_2 {#2} + \theta_3 {#3} + \theta_4 {#4} + \theta_5 {#5}}
\newcommand*{\fe}[5]{$\foe{#1}{#2}{#3}{#4}{#5}$}

\begin{document}
\maketitle

\section{Linear regression and Logistic regression}
\subsection{Task 1}
In order to experiment with linear regression I have chosen to use the following 8 features, referred to in equations as $a$ through $h$ for brevity:\\

\begin{tabular}{l l}
	$a$	& stock volume of previous day \\
	$b$	& difference between the previous two days' stock volumes \\
	$c$	& mean of stock volumes from previous ten days \\
	$d$	& standard deviation of stock volumes from previous ten days \\
	$e$	& stock price of previous day \\
	$f$	& difference between the previous two days' stock prices \\
	$g$	& mean of stock prices from previous ten days \\
	$h$	& standard deviation of stock prices from previous ten days
\end{tabular}\\

Further, the elements of a vector $\theta$ represent the coefficients of a regression function, with $\theta_0$ always representing the constant term.  Hence, a regression function might look as follows:

\[ \foc{a}{a^2}{b} \]

Figure \ref{task1onetwoorder} shows the Mean Squared Errors (MSEs) obtained in my initial phase of experimentation with the chosen features, shown to three significant figures.  In this phase of experimentation, I evaluated the performance of each feature used on its own in first- and second-order polynomials.

\begin{figure}
\centering
\begin{bchart}[step=20,max=80]
	\bcbar[label={\fa{a}}]{45.2}
		\smallskip
	\bcbar[label={\fa{b}}]{68.4}
		\smallskip
	\bcbar[label={\fa{c}}]{36.5}
		\smallskip
	\bcbar[label={\fa{d}}]{49.8}
		\smallskip
	\bcbar[label={\fa{e}}]{49.9}
		\smallskip
	\bcbar[label={\fa{f}}]{70.2}
		\smallskip
	\bcbar[label={\fa{g}}]{1.63}
		\smallskip
	\bcbar[label={\fa{h}}]{25.7}
		\smallskip
	\bcbar[label={\fb{a}{a^2}}]{43.9}
		\smallskip
	\bcbar[label={\fb{b}{b^2}}]{44.7}
		\smallskip
	\bcbar[label={\fb{c}{c^2}}]{35.9}
		\smallskip
	\bcbar[label={\fb{d}{d^2}}]{50.9}
		\smallskip
	\bcbar[label={\fb{e}{e^2}}]{0.382}
		\smallskip
	\bcbar[label={\fb{f}{f^2}}]{55.8}
		\smallskip
	\bcbar[label={\fb{g}{g^2}}]{1.55}
		\smallskip
	\bcbar[label={\fb{h}{h^2}}]{23.1}
\end{bchart}
\caption{MSEs obtained using each of the features on their own in first- and second-order polynomials}
\label{task1onetwoorder}
\end{figure}

It is clear from these results that the best performances come when using the features that relate to stock price as opposed to stock value ($e$ -- $h$).  However, feature $f$, the difference between the last two days' stock prices does not appear to perform very well.  Feature $e$ does not perform well as a first-order polynomial, but is exceptionally good as a second-order polynomial.

My next phase of experimentation is to take the high-performing features and try using them on their own in third-order polynomial functions.  Following this, I will experiment with combining the better performing features to see what improvements can be made.  The results of the initial third-order polynomial experiments are shown in figure \ref{task1threeorder}.  Out of interest, I have chosen to try feature $c$ as a third-order polynomial as it was the best performing of the stock volume-related features.

\begin{figure}
\centering
\begin{bchart}[step=20,max=80]
	\bcbar[label={\fc{c}{c^2}{c^3}}]{32.8}
		\smallskip
	\bcbar[label={\fc{e}{e^2}{e^3}}]{0.376}
		\smallskip
	\bcbar[label={\fc{g}{g^2}{g^3}}]{1.65}
		\smallskip
	\bcbar[label={\fc{h}{h^2}{h^3}}]{23.0}
\end{bchart}
\caption{MSEs obtained using selected features on their own in three-order polynomials}
\label{task1threeorder}
\end{figure}

As seen by the results, each feature tested in third-order polynomials had very similar results to the second-order polynomial tests.  Going forward, I have chosen to try a function combining $e$ and $g$ both as second-order polynomials to see if they perform well as a pair.  I am also interested to to see if the recent change in stock volume ($b$) combined with the mean of the last ten days' stock prices ($g$) gives an indication of the next stock price.  Further, combining $a$, $b$, $d$ and $g$ all together may give good results.  The MSEs obtained for these tests are shown in figure \ref{task1complex} (note the change of scale for this chart).

\begin{figure}
\centering
\begin{bchart}[step=1,max=2]
	\bcbar[label={\fd{e}{e^2}{g}{g^2}}]{0.392}
		\smallskip
	\bcbar[label={\fd{b}{b^2}{g}{g^2}}]{1.56}
		\smallskip
	\bcbar[label={\fe{a}{b}{d}{g}{g^2}}]{1.46}
\end{bchart}
\caption{MSEs obtained when combining features into more complex polynomials}
\label{task1complex}
\end{figure}

From these results we can see that combining $e$ and $g$ does not really improve on the performance achieved when using $e$ alone.  Also, combining $g$ variously with $a$, $b$ and $d$ does not improve on the performance of using $g$ alone.

\subsection{Task 2}

The same features, represented as $a$ through $h$, are used in this task.  For simplicity, I will simply list the features and powers used in each test (for example, $a,a^2,b$).  The constant term is implicitly used in every case.

Again, I started by evaluating the use of each feature on its own in first- and second-order polynomials.

\begin{figure}
\centering
\begin{bchart}[step=20,max=100]
	\bcbar[label={$a$}]{30.6}
		\smallskip
	\bcbar[label={$b$}]{37.0}
		\smallskip
	\bcbar[label={$c$}]{39.6}
		\smallskip
	\bcbar[label={$d$}]{27.2}
		\smallskip
	\bcbar[label={$e$}]{30.8}
		\smallskip
	\bcbar[label={$f$}]{45.6}
		\smallskip
	\bcbar[label={$g$}]{27.2}
		\smallskip
	\bcbar[label={$h$}]{45.5}
		\smallskip
	\bcbar[label={$a,a^2$}]{39.9}
		\smallskip
	\bcbar[label={$b,b^2$}]{12.9}
		\smallskip
	\bcbar[label={$c,c^2$}]{29.4}
		\smallskip
	\bcbar[label={$d,d^2$}]{30.4}
		\smallskip
	\bcbar[label={$e,e^2$}]{35.3}
		\smallskip
	\bcbar[label={$f,f^2$}]{45.7}
		\smallskip
	\bcbar[label={$g,g^2$}]{26.4}
		\smallskip
	\bcbar[label={$h,h^2$}]{45.6}
\end{bchart}
\caption{\% accuracies obtained using each of the features on their own in first- and second-order polynomials}
\label{task2onetwoorder}
\end{figure}

Figure \ref{task2onetwoorder} shows the percentage accuracy achieved by each of the tests using each feature individually in first- and second-order polynomials.  As can be seen, the high performing features ($f$, $h$) did not benefit from being included in a squared form.  Some of the features were better used in their second-order polynomials than in their first-order ones (e.g. $a$) while others were significantly worse (e.g. $b$).

I now will test the performance of using different combinations of features.  One such combination is $f,h$, to see if combining them improves on their individual successes.  Also, the pairs $e,h$ and $g,h$, since each pair can give an idea for the general spread of recent stock price data in terms of percentage.  The effect of relationship between $c,g$ is also interesting to test, as well as the quadruple of $a,c,e,g$, since it includes information about the relationship between the last day's data and the mean data from the last 10 days.  In each test, each feature will be raised to the power at which it performed best from the previous tests.

\begin{figure}
\centering
\begin{bchart}[step=20,max=100]
	\bcbar[label={$f,h$}]{45.5}
		\smallskip
	\bcbar[label={$e,e^2,h$}]{24.6}
		\smallskip
	\bcbar[label={$g,h$}]{25.7}
		\smallskip
	\bcbar[label={$c,g$}]{38.4}
		\smallskip
	\bcbar[label={$a,a^2,c,e,e^2,g$}]{45.0}
		\smallskip
	\bcbar[label={$a,a^2,c,g$}]{44.1}
\end{bchart}
\caption{\% accuracies obtained when combining features into more complex polynomials}
\label{task2complex}
\end{figure}

As with task 1, it is seen from the results that combining features did not really improve the performance.  Features $f$ and $h$ perform as well individually as when combined together, for example.  The $e,e^2,h$ result is worse than all of the $e$ and $h$ independent results, which is also true for the $g,h$ result.  $c,g$ performed only as well as $c$ on its own.  $a,a^2,c,e,e^2,g$ is the only combination that performs better as a group than any of its constituents on its own.  After the relative success of $a,a^2,c,e,e^2,g$, I chose to try removing $e$ in case it was dragging the success down.  As can be seen, this did not give any improvement.

\subsection{Task 3}

\section{Bayesian networks}

\subsection{Task 5 Question 1}

A conditional probability $P(A|B)$ could be estimated by selecting all of the instantiations that satisfy the condition $B$ and calculating the proportion of those for which $A$ is true.

For example, if we want to estimate $P(7=1|6=1)$ (The probability that node 7 is set to 1 given that node 6 is set to 1) then we produce a sample and select from it all of the instantiations in which node 6 is set to 1.  Figure \ref{sample1} shows a full sample of 10 instantiations, with the selected instantiations marked using an asterisk (*).

\begin{figure}[h]
	\centering
	\lstset{basicstyle=\ttfamily}
	\begin{tabular}{c}
	\begin{lstlisting}
[1, 0, 0, 0, 0, 0, 1, 1] *
[0, 0, 1, 0, 0, 0, 1, 1] *
[1, 0, 1, 0, 0, 0, 1, 1] *
[0, 0, 0, 0, 0, 0, 1, 0] *
[1, 0, 0, 0, 0, 0, 1, 0] *
[1, 0, 1, 0, 0, 0, 1, 0] *
[0, 0, 1, 0, 0, 0, 1, 1] *
[0, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 1, 0] *
[1, 0, 1, 0, 0, 0, 1, 1] *
	\end{lstlisting}
	\end{tabular}
	\caption{A sample of size 10, with the instantiations where node 6 is set to 1 marked by asterisks}
	\label{sample1}
\end{figure}

From the selection, we find out how many instantiations also have node 7 set to 1. Figure \ref{sample2} shows just the 9 selected rows, and marks the rows in which node 7 is set to 1 with a plus sign (+).

\begin{figure}[h]
	\centering
	\lstset{basicstyle=\ttfamily}
	\begin{tabular}{c}
	\begin{lstlisting}
[1, 0, 0, 0, 0, 0, 1, 1] +
[0, 0, 1, 0, 0, 0, 1, 1] +
[1, 0, 1, 0, 0, 0, 1, 1] +
[0, 0, 0, 0, 0, 0, 1, 0] 
[1, 0, 0, 0, 0, 0, 1, 0] 
[1, 0, 1, 0, 0, 0, 1, 0] 
[0, 0, 1, 0, 0, 0, 1, 1] +
[0, 0, 0, 0, 0, 0, 1, 0] 
[1, 0, 1, 0, 0, 0, 1, 1] +
	\end{lstlisting}
	\end{tabular}
	\caption{The selected rows from figure \ref{sample1}, with the instantiations where node 7 is set to 1 marked by plus signs}
	\label{sample2}
\end{figure}

We can see that, of the 9 instantiations that satisfy $6=1$, 5 also satisfy $7=1$.  We therefore estimate that $P(7=1|6=1) = \frac{5}{9}$.

\subsection{Task 5 Question 2}

\textbf{I have yet to answer this question.}

\subsection{Task 5 Question 3}

I will use the same example in this question as in question 1 (find $P(7=1|6=1)$).  From the original data provided in bndata.csv, we know that the probability is $3688 / 8887 = 0.415$ (3dp).  Using the method described above, I will estimate the probability based on 100, 1000, 5000 and 10,000 samples and record them in the table below.  The final column shows the percentage error of the estimation.

\begin{tabular}{l l l l l}
	Samples		& Number where 6 = 1	& Number where 7 = 1	& Estimated probability (5dp)		& Error (3sf) \\
	\hline
	100		& 87	& 28	& 0.32184		& 22.4 \\
	1000	& 885	& 360	& 0.40678		& 0.0198 \\
	5000	& 4450	& 1834	& 0.41213		& 0.00692 \\
	10,000	& 8888	& 3691	& 0.41528		& 0.000675
\end{tabular}

Here, the accuracy of the estimation is shown to get gradually better and better as the number of samples increases from 100 to 10,000.

\end{document}